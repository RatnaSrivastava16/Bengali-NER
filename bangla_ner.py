# -*- coding: utf-8 -*-
"""Bangla NER

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vCSIKnVWgLXFZemCwbc7DCWzI__F8vKa
"""

import pandas as pd
import numpy as np

data=pd.read_csv("/content/drive/MyDrive/b-ner.csv")
data.head()

data.drop(columns=["Sentence #"],inplace=True)

data['Tag'].value_counts()

data.info()

import pandas as pd
import re
import nltk
from nltk.tokenize import word_tokenize
from sklearn.model_selection import train_test_split

# Make sure to download the punkt tokenizer if not already done
nltk.download('punkt')

# Load the dataset
# Assuming the dataset is in a CSV format with columns 'text' and 'tag'
# data = pd.read_csv('bengali_dataset.csv')  # Replace with your dataset path

# Display the first few rows of the dataset
print(data.head())

# Data Cleaning Function
def clean_text(text):
    # Remove unwanted characters, links, etc.
    text = re.sub(r'https?://\S+|www\.\S+', '', text)  # Remove URLs
    text = re.sub(r'@[A-Za-z0-9_]+', '', text)  # Remove @mentions
    text = re.sub(r'#', '', text)  # Remove hashtags
    text = re.sub(r'[^a-zA-Z0-9\u0980-\u09FF\s]', '', text)  # Remove non-Bengali characters and special characters
    text = text.strip()  # Remove leading and trailing whitespace
    return text

# Apply cleaning to the 'text' column
data['Word'] = data['Word'].apply(clean_text)

# Tokenization Function
def tokenize_text(text):
    return word_tokenize(text)

# Apply tokenization
data['tokens'] = data['Word'].apply(tokenize_text)

# Display the cleaned and tokenized data
print(data[['Word', 'tokens']].head())

# Prepare data for model training
# Assuming your NER tagging is in a separate column 'tag'
# Flatten the tokens and tags for training
flattened_data = []

for index, row in data.iterrows():
    for token, tag in zip(row['tokens'], row['Tag'].split()):  # Adjust based on your tag format
        flattened_data.append((token, tag))

# Create a DataFrame for the flattened data
flattened_df = pd.DataFrame(flattened_data, columns=['token', 'tag'])

# Split the dataset into training and testing sets
train_data, test_data = train_test_split(flattened_df, test_size=0.2, random_state=42)

# Display the training and testing data
print("Training data:")
print(train_data.head())
print("Testing data:")
print(test_data.head())

from gensim.models import Word2Vec
import numpy as np

# Create a list of sentences from the tokens
sentences = data['tokens'].tolist()

# Train the Word2Vec model
word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)

# Function to convert tokens to vectors
def vectorize_tokens(tokens):
    vectors = []
    for token in tokens:
        if token in word2vec_model.wv:
            vectors.append(word2vec_model.wv[token])
        else:
            vectors.append(np.zeros(word2vec_model.vector_size))  # Use zero vector for unknown tokens
    return vectors

train_vectors = train_data['token'].apply(vectorize_tokens).tolist()
test_vectors = test_data['token'].apply(vectorize_tokens).tolist()

train_data

data['Tag'].unique()

# dict1={'B-geo':1,'O':2,'B-gpe':3,'B-per':4,'I-per':5,'B-org':6,'I-org':7,'I-geo':8,'B-tim':9,'I-tim':10,'I-gpe':11,'B-nat':12,'I-nat':13}
# train_data['Tag']=train_data['tag'].map(dict1)
# test_data['Tag']=test_data['tag'].map(dict1)

# Step 1: Create a mapping for tags to indices and vice versa
tags = list(data['Tag'].unique())  # Get a list of all unique tags
tag_to_index = {tag: idx for idx, tag in enumerate(tags)}
index_to_tag = {idx: tag for tag, idx in tag_to_index.items()}

# Step 2: Convert tags to integer sequences using the mapping
def convert_tags_to_indices(tags_list):
    return [tag_to_index[tag] for tag in tags_list]

# Convert tags in training and test data to indices
train_data['tag_index'] = train_data['tag'].apply(convert_tags_to_indices)
test_data['tag_index'] = test_data['tag'].apply(convert_tags_to_indices)

# Step 3: Pad the tag sequences to ensure uniform length
y_train_padded = pad_sequences(train_data['tag_index'].tolist(), maxlen=max_length, padding='post', value=tag_to_index['O'])
y_test_padded = pad_sequences(test_data['tag_index'].tolist(), maxlen=max_length, padding='post', value=tag_to_index['O'])

# Step 4: Convert y_train and y_test to one-hot encoded vectors for multi-class classification
y_train = [to_categorical(seq, num_classes=len(tag_to_index)) for seq in y_train_padded]
y_test = [to_categorical(seq, num_classes=len(tag_to_index)) for seq in y_test_padded]

# Convert to numpy arrays for training
X_train = np.array(X_train)
X_test = np.array(X_test)
y_train = np.array(y_train)
y_test = np.array(y_test)

# Now, X_train, y_train, X_test, and y_test are ready for model training.

from keras.models import Sequential
from keras.layers import LSTM, Dense, TimeDistributed, Dropout

# Define the LSTM model
model = Sequential()
model.add(LSTM(units=128, return_sequences=True, input_shape=(max_length, 100)))  # 100 is the word vector size
model.add(Dropout(0.5))
model.add(TimeDistributed(Dense(len(tag_to_index), activation='softmax')))  # TimeDistributed for sequence labeling
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# Train the model
history = model.fit(X_train, y_train, batch_size=32, epochs=10, validation_split=0.1, verbose=1)

# Evaluate the model on the test data
loss, accuracy = model.evaluate(X_test, y_test, verbose=1)
print(f'Test Loss: {loss:.4f}')
print(f'Test Accuracy: {accuracy:.4f}')

